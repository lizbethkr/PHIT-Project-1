{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "The data used in this notebook is sourced from the National Centers for Environmental Information (NCEI): [Global Historical Climatology Network (GHCN) - Hourly](https://www.ncei.noaa.gov/products/global-historical-climatology-network-hourly). Refer to their documentation and terms of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set\n",
    "\n",
    "Station_ID: the station identification code. The first two characters signify the FIPS country code, the third character is a network code identifying the station numbering system used, and the remaining eight characters contain the actual station ID.\n",
    "\n",
    "Station_Name: the name of the station.\n",
    "\n",
    "Year: the year the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Month: the month the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Day: the day the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Hour: the hour the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Latitude: latitude of the station (in decimal degrees). North (+); South (-).\n",
    "\n",
    "Longitude: the longitude of the station (in decimal degrees). East (+); West (-).\n",
    "\n",
    "Temperature: 2 meter (circa) Above Ground Level Air (dry bulb) Temperature (‚Å∞C to tenths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- Raw data was removed in download_ghcn.py for storage purposes.\n",
    "- GHCN hourly dataset contained psv files for individual stations in specific years. When processing the data, it was converted to csv format files for all California stations in years 2003 - 2023.\n",
    "- Most columns were dropped as they were not needed. Columns kept were described above.\n",
    "- Duplicate rows that had completely same column values were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: reduced_CA_stations_2003.csv\n",
      "Processed file: reduced_CA_stations_2004.csv\n",
      "Processed file: reduced_CA_stations_2005.csv\n",
      "Processed file: reduced_CA_stations_2006.csv\n",
      "Processed file: reduced_CA_stations_2007.csv\n",
      "Processed file: reduced_CA_stations_2008.csv\n",
      "Processed file: reduced_CA_stations_2009.csv\n",
      "Processed file: reduced_CA_stations_2010.csv\n",
      "Processed file: reduced_CA_stations_2011.csv\n",
      "Processed file: reduced_CA_stations_2012.csv\n",
      "Processed file: reduced_CA_stations_2013.csv\n",
      "Processed file: reduced_CA_stations_2014.csv\n",
      "Processed file: reduced_CA_stations_2015.csv\n",
      "Processed file: reduced_CA_stations_2016.csv\n",
      "Processed file: reduced_CA_stations_2017.csv\n",
      "Processed file: reduced_CA_stations_2018.csv\n",
      "Processed file: reduced_CA_stations_2019.csv\n",
      "Processed file: reduced_CA_stations_2020.csv\n",
      "Processed file: reduced_CA_stations_2021.csv\n",
      "Processed file: reduced_CA_stations_2022.csv\n",
      "Processed file: reduced_CA_stations_2023.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Update paths to get source code from notebook_utils\n",
    "curr_dir = os.path.dirname(os.path.abspath('notebooks'))\n",
    "proj_dir = os.path.dirname(curr_dir)\n",
    "src_path = os.path.join(proj_dir, 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from notebook_utils.preprocessing import *\n",
    "\n",
    "CA_stations = get_reduced_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_ID</th>\n",
       "      <th>Station_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USW00023225</td>\n",
       "      <td>BLUE CANYON NYACK AP</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.2761</td>\n",
       "      <td>-120.7092</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USW00023225</td>\n",
       "      <td>BLUE CANYON NYACK AP</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.2761</td>\n",
       "      <td>-120.7092</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USW00023225</td>\n",
       "      <td>BLUE CANYON NYACK AP</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>39.2761</td>\n",
       "      <td>-120.7092</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USW00023225</td>\n",
       "      <td>BLUE CANYON NYACK AP</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>39.2761</td>\n",
       "      <td>-120.7092</td>\n",
       "      <td>-1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USW00023225</td>\n",
       "      <td>BLUE CANYON NYACK AP</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>39.2761</td>\n",
       "      <td>-120.7092</td>\n",
       "      <td>-2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Station_ID          Station_name  Year  Month  Day  Hour  Latitude  \\\n",
       "0  USW00023225  BLUE CANYON NYACK AP  2003      1    1     0   39.2761   \n",
       "1  USW00023225  BLUE CANYON NYACK AP  2003      1    1     1   39.2761   \n",
       "2  USW00023225  BLUE CANYON NYACK AP  2003      1    1     2   39.2761   \n",
       "3  USW00023225  BLUE CANYON NYACK AP  2003      1    1     3   39.2761   \n",
       "4  USW00023225  BLUE CANYON NYACK AP  2003      1    1     4   39.2761   \n",
       "\n",
       "   Longitude  temperature  \n",
       "0  -120.7092         -1.1  \n",
       "1  -120.7092         -1.1  \n",
       "2  -120.7092         -1.1  \n",
       "3  -120.7092         -1.7  \n",
       "4  -120.7092         -2.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CA_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19851938 entries, 0 to 19851937\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   Station_ID    object \n",
      " 1   Station_name  object \n",
      " 2   Year          int64  \n",
      " 3   Month         int64  \n",
      " 4   Day           int64  \n",
      " 5   Hour          int64  \n",
      " 6   Latitude      float64\n",
      " 7   Longitude     float64\n",
      " 8   temperature   float64\n",
      "dtypes: float64(3), int64(4), object(2)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "CA_stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.985194e+07</td>\n",
       "      <td>1.985194e+07</td>\n",
       "      <td>1.985194e+07</td>\n",
       "      <td>1.985194e+07</td>\n",
       "      <td>1.985194e+07</td>\n",
       "      <td>1.985194e+07</td>\n",
       "      <td>1.961237e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.013089e+03</td>\n",
       "      <td>6.504389e+00</td>\n",
       "      <td>1.573293e+01</td>\n",
       "      <td>1.159572e+01</td>\n",
       "      <td>3.668450e+01</td>\n",
       "      <td>-1.201430e+02</td>\n",
       "      <td>1.561475e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.375401e+00</td>\n",
       "      <td>3.467041e+00</td>\n",
       "      <td>8.794091e+00</td>\n",
       "      <td>6.939050e+00</td>\n",
       "      <td>2.374357e+00</td>\n",
       "      <td>2.021136e+00</td>\n",
       "      <td>8.620436e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.003000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.256810e+01</td>\n",
       "      <td>-1.242381e+02</td>\n",
       "      <td>-9.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.441420e+01</td>\n",
       "      <td>-1.218150e+02</td>\n",
       "      <td>1.050000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.013000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.698500e+01</td>\n",
       "      <td>-1.204667e+02</td>\n",
       "      <td>1.470000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.018000e+03</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>3.832080e+01</td>\n",
       "      <td>-1.182911e+02</td>\n",
       "      <td>2.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.023000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>4.178360e+01</td>\n",
       "      <td>-1.161472e+02</td>\n",
       "      <td>9.020000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Year         Month           Day          Hour      Latitude  \\\n",
       "count  1.985194e+07  1.985194e+07  1.985194e+07  1.985194e+07  1.985194e+07   \n",
       "mean   2.013089e+03  6.504389e+00  1.573293e+01  1.159572e+01  3.668450e+01   \n",
       "std    5.375401e+00  3.467041e+00  8.794091e+00  6.939050e+00  2.374357e+00   \n",
       "min    2.003000e+03  1.000000e+00  1.000000e+00  0.000000e+00  3.256810e+01   \n",
       "25%    2.009000e+03  3.000000e+00  8.000000e+00  6.000000e+00  3.441420e+01   \n",
       "50%    2.013000e+03  7.000000e+00  1.600000e+01  1.200000e+01  3.698500e+01   \n",
       "75%    2.018000e+03  1.000000e+01  2.300000e+01  1.800000e+01  3.832080e+01   \n",
       "max    2.023000e+03  1.200000e+01  3.100000e+01  2.300000e+01  4.178360e+01   \n",
       "\n",
       "          Longitude   temperature  \n",
       "count  1.985194e+07  1.961237e+07  \n",
       "mean  -1.201430e+02  1.561475e+01  \n",
       "std    2.021136e+00  8.620436e+00  \n",
       "min   -1.242381e+02 -9.900000e+01  \n",
       "25%   -1.218150e+02  1.050000e+01  \n",
       "50%   -1.204667e+02  1.470000e+01  \n",
       "75%   -1.182911e+02  2.000000e+01  \n",
       "max   -1.161472e+02  9.020000e+02  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CA_stations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature column has a really high max celsius value which is 902 degrees celsius. This is unreasonably high. After doing some searching, we found that the highest recorded temperature value was 56.7 degrees celsius in California 1913. \n",
    "\n",
    "There is also an unreasonably low temperature observation of -99 degrees celsius since the lowest recorded temperature observation on Earth was -98 degrees in Antartica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19851938 entries, 0 to 19851937\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   Station_ID    object \n",
      " 1   Station_name  object \n",
      " 2   Year          int16  \n",
      " 3   Month         int8   \n",
      " 4   Day           int8   \n",
      " 5   Hour          int8   \n",
      " 6   Latitude      float32\n",
      " 7   Longitude     float32\n",
      " 8   temperature   float32\n",
      "dtypes: float32(3), int16(1), int8(3), object(2)\n",
      "memory usage: 624.8+ MB\n"
     ]
    }
   ],
   "source": [
    "optimize_col_types(CA_stations)\n",
    "CA_stations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Invalid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle Missing Values  (e.g., mean/median impuation, interpolation, forward or backward fill, k-nearest neighbors imputation, deletion)\n",
    "- Handle Outliers  (e.g., visual inspection by boxplots, Z-score and IQR method, or data transformation by log transformation and winsorization)\n",
    "- Handle inconsistencies (e.g., checking ranges to ensure temperature values fall within a reasonable range, unit consistency, string matching and standardization), and duplicates (identify and remove duplicates) in the dataset\n",
    "\n",
    "Notes:\n",
    "- For non-leap years, there should be 8760 rows (for each hour) for each station.\n",
    "- For leap years, there should be 8784 rows (for each hour) for each station\n",
    "- Leap years from 2003-2023 include: 2004, 2008, 2012, 2016, and 2020\n",
    "- The reduced files contain 99 CA stations.\n",
    "- Some stations are not observed each year from 2003-2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure temperature observations are within -50¬∞C and 60¬∞C\n",
    "2. Temperature values above the reasonable range will be converted to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_ID</th>\n",
       "      <th>Station_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1439248</th>\n",
       "      <td>USW00093104</td>\n",
       "      <td>CHINA LAKE NAF</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>35.686401</td>\n",
       "      <td>-117.690804</td>\n",
       "      <td>80.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440726</th>\n",
       "      <td>USW00093104</td>\n",
       "      <td>CHINA LAKE NAF</td>\n",
       "      <td>2005</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>35.686401</td>\n",
       "      <td>-117.690804</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448841</th>\n",
       "      <td>USW00093107</td>\n",
       "      <td>SAN DIEGO MIRAMAR NAS</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>32.866699</td>\n",
       "      <td>-117.133301</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457459</th>\n",
       "      <td>USW00093107</td>\n",
       "      <td>SAN DIEGO MIRAMAR NAS</td>\n",
       "      <td>2005</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>32.866699</td>\n",
       "      <td>-117.133301</td>\n",
       "      <td>-78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458755</th>\n",
       "      <td>USW00093107</td>\n",
       "      <td>SAN DIEGO MIRAMAR NAS</td>\n",
       "      <td>2005</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>32.866699</td>\n",
       "      <td>-117.133301</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19654066</th>\n",
       "      <td>USW00023289</td>\n",
       "      <td>PALO ALTO</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>37.466702</td>\n",
       "      <td>-122.116699</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19772424</th>\n",
       "      <td>USW00093193</td>\n",
       "      <td>FRESNO YOSEMITE INTL</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>36.779999</td>\n",
       "      <td>-119.720299</td>\n",
       "      <td>227.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19781493</th>\n",
       "      <td>USW00093201</td>\n",
       "      <td>TRUCKEE AP</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>39.320000</td>\n",
       "      <td>-120.139397</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19829035</th>\n",
       "      <td>USW00093231</td>\n",
       "      <td>SAN CARLOS AP</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>37.516701</td>\n",
       "      <td>-122.250000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19829095</th>\n",
       "      <td>USW00093231</td>\n",
       "      <td>SAN CARLOS AP</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>37.516701</td>\n",
       "      <td>-122.250000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4283 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Station_ID           Station_name  Year  Month  Day  Hour  \\\n",
       "1439248   USW00093104         CHINA LAKE NAF  2005      3   21    12   \n",
       "1440726   USW00093104         CHINA LAKE NAF  2005      5   20     2   \n",
       "1448841   USW00093107  SAN DIEGO MIRAMAR NAS  2005      3   16    11   \n",
       "1457459   USW00093107  SAN DIEGO MIRAMAR NAS  2005      9   23     2   \n",
       "1458755   USW00093107  SAN DIEGO MIRAMAR NAS  2005     10   21    22   \n",
       "...               ...                    ...   ...    ...  ...   ...   \n",
       "19654066  USW00023289              PALO ALTO  2023      2    8    17   \n",
       "19772424  USW00093193   FRESNO YOSEMITE INTL  2023      5   15    16   \n",
       "19781493  USW00093201             TRUCKEE AP  2023      5   23    15   \n",
       "19829035  USW00093231          SAN CARLOS AP  2023      2   10    15   \n",
       "19829095  USW00093231          SAN CARLOS AP  2023      2   12    20   \n",
       "\n",
       "           Latitude   Longitude  temperature  \n",
       "1439248   35.686401 -117.690804    80.300003  \n",
       "1440726   35.686401 -117.690804    83.000000  \n",
       "1448841   32.866699 -117.133301    78.000000  \n",
       "1457459   32.866699 -117.133301   -78.000000  \n",
       "1458755   32.866699 -117.133301    78.000000  \n",
       "...             ...         ...          ...  \n",
       "19654066  37.466702 -122.116699    80.000000  \n",
       "19772424  36.779999 -119.720299   227.000000  \n",
       "19781493  39.320000 -120.139397    70.000000  \n",
       "19829035  37.516701 -122.250000    70.000000  \n",
       "19829095  37.516701 -122.250000   170.000000  \n",
       "\n",
       "[4283 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many rows are below -50 and above 60 degrees celsius\n",
    "CA_stations[(CA_stations['temperature'] < -50) | (CA_stations['temperature'] > 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to turn temperature outside of the specified range into NaN\n",
    "CA_stations.loc[(CA_stations['temperature'] < -50) | (CA_stations['temperature'] > 60), 'temperature'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are rows with the same value in each column except temperature. In these cases we will average out the temperature observations and delete the extra rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = CA_stations.groupby(['Station_ID', 'Station_name', 'Year', 'Month', 'Day', 'Hour', 'Latitude', 'Longitude']).agg({'temperature': 'mean'}).reset_index()\n",
    "CA_stations = grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    }
   ],
   "source": [
    "# create a reference dataframe with all stations with hours from 2003 to 2023\n",
    "full_df = create_full_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in completely missing rows\n",
    "missing_rows = find_missing_rows(full_df, CA_stations)\n",
    "CA_stations = add_missing_rows(CA_stations, missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = check_station_rows(CA_stations)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations with no Station_name values will be dropped as that means there are no observations recorded for them at all from 2003 - 2023\n",
    "- This is because the above Station_name for missing rows were filled in by using the Station_name used from a filled column with the same Station_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.dropna(subset=['Station_name'], inplace=True)   # 21 stations dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = CA_stations.isna().sum()\n",
    "null_percent = (null_count/CA_stations.shape[0]) * 100\n",
    "print(f'Percentage of null values for each column: \\n{null_percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Temperature Values: Handling large gaps in temperature observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a \"large gap\" as being a gap in the temperature column that is more than a day.\n",
    "\n",
    "Large gaps in temperature observations will be handled via interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubic Spline Interpolation for large gaps of missing temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.preprocessing import cubic_spline_interpolate\n",
    "\n",
    "CA_interpolated_df = cubic_spline_interpolate(CA_stations,gap_hours=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_interpolated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = CA_interpolated_df.isna().sum()\n",
    "null_percent = (null_count/CA_interpolated_df.shape[0]) * 100\n",
    "print(f'Percentage of null values for each column: \\n{null_percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Temperature Values: Handling short gaps in temperature observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a \"short gap\" as being a gap in the temperature column that is less than a day. \n",
    "\n",
    "Short gaps in temperature observations will be handled with forward/backward fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = fill_gaps(CA_interpolated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = final_df.isna().sum()\n",
    "null_percent = (null_count/final_df.shape[0]) * 100\n",
    "print(f'Percentage of null values for each column: \\n{null_percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make copies to not alter original dataframes\n",
    "plot_CA_stations = CA_stations.copy()\n",
    "plot_CA_interpolated = CA_interpolated_df.copy()\n",
    "\n",
    "plot_CA_stations['Datetime'] = pd.to_datetime(plot_CA_stations[['Year', 'Month', 'Day', 'Hour']])\n",
    "plot_CA_interpolated['Datetime'] = pd.to_datetime(plot_CA_interpolated[['Year', 'Month', 'Day', 'Hour']])\n",
    "\n",
    "plot_CA_stations.set_index('Datetime', inplace=True)\n",
    "plot_CA_interpolated.set_index('Datetime', inplace=True)\n",
    "\n",
    "monthly_avg_original = plot_CA_stations['temperature'].resample('ME').mean()\n",
    "monthly_avg_interpolated = plot_CA_interpolated['temperature'].resample('ME').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(monthly_avg_original.index, monthly_avg_original.values, 'o-', label='Original Data', markersize=4)\n",
    "\n",
    "plt.plot(monthly_avg_interpolated.index, monthly_avg_interpolated.values, 'x-', label='Interpolated Data', markersize=4)\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Temperature (¬∞C)')\n",
    "plt.title('Monthly Average Temperature (2003-2023)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers can skew our statistical analysis of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Visual Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Year', y='temperature', data=CA_interpolated_df)\n",
    "plt.title('Temperature Boxplot by Year')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.ylabel('Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Statistical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# z_stations dataframe will have a Z_score column added to it\n",
    "z_stations = CA_interpolated_df.copy()\n",
    "z_stations['Z_score'] = stats.zscore(z_stations['temperature'])\n",
    "z_thresh = 3\n",
    "\n",
    "# calculate outliers using Z-score\n",
    "z_outliers = z_stations[(z_stations['Z_score'] < -z_thresh) | (z_stations['Z_score'] > z_thresh)]\n",
    "total_observations = z_stations.shape[0]\n",
    "\n",
    "num_outliers = z_outliers.shape[0]\n",
    "\n",
    "percent_outliers = (num_outliers / total_observations) * 100\n",
    "\n",
    "print(f'{percent_outliers:.2f}% of the observations are outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stations_cleaned = z_stations[~((z_stations['Z_score'] < -z_thresh) | (z_stations['Z_score'] > z_thresh))]\n",
    "\n",
    "# Drop the Z_score column as it's no longer needed\n",
    "z_stations_cleaned = z_stations_cleaned.drop(columns=['Z_score'])\n",
    "\n",
    "# The z_stations_cleaned dataframe now contains the data without the outliers\n",
    "z_stations_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTC to Local Time Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stations_cleaned['Datetime'] = pd.to_datetime(z_stations_cleaned[['Year', 'Month', 'Day', 'Hour']])\n",
    "z_stations_cleaned = z_stations_cleaned.drop(columns=['Year', 'Month', 'Day', 'Hour'])\n",
    "\n",
    "z_stations_cleaned['Datetime_local'] = z_stations_cleaned['Datetime'].dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
    "z_stations_cleaned['Year'] = z_stations_cleaned['Datetime_local'].dt.year\n",
    "z_stations_cleaned['Month'] = z_stations_cleaned['Datetime_local'].dt.month\n",
    "z_stations_cleaned['Day'] = z_stations_cleaned['Datetime_local'].dt.day\n",
    "z_stations_cleaned['Hour'] = z_stations_cleaned['Datetime_local'].dt.hour\n",
    "z_stations_cleaned = z_stations_cleaned.drop(columns=['Datetime', 'Datetime_local'])\n",
    "\n",
    "# remove 2002 observations\n",
    "z_stations_cleaned = z_stations_cleaned[z_stations_cleaned['Year'] != 2002]\n",
    "\n",
    "# rearrange columns\n",
    "cols = ['Station_ID','Station_name', 'Latitude', 'Longitude','Year','Month','Day','Hour','temperature']\n",
    "z_stations_cleaned = z_stations_cleaned[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stations_cleaned.head()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to csv files by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../data/processed/ghcn_clean'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "years = z_stations_cleaned['Year'].unique()\n",
    "\n",
    "# Capitalize the temperature column to match the rest.\n",
    "z_stations_cleaned = z_stations_cleaned.rename(columns={'temperature': 'Temperature'})\n",
    "z_stations_cleaned.sort_values(by=['Station_ID', 'Year', 'Month', 'Day', 'Hour'], inplace=True)\n",
    "\n",
    "for year in years:\n",
    "    yearly_data = z_stations_cleaned[z_stations_cleaned['Year'] == year]\n",
    "    output_file = os.path.join(output_folder, f'CA_{year}_clean.csv')\n",
    "    yearly_data.to_csv(output_file, index=False)\n",
    "\n",
    "print('New datafrane yearly files saved to ghcn_cleaned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
