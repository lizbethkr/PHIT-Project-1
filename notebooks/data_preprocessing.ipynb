{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "The data used in this notebook is sourced from the National Centers for Environmental Information (NCEI): [Global Historical Climatology Network (GHCN) - Hourly](https://www.ncei.noaa.gov/products/global-historical-climatology-network-hourly). Refer to their documentation and terms of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set\n",
    "\n",
    "Station_ID: the station identification code. The first two characters signify the FIPS country code, the third character is a network code identifying the station numbering system used, and the remaining eight characters contain the actual station ID.\n",
    "\n",
    "Station_Name: the name of the station.\n",
    "\n",
    "Year: the year the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Month: the month the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Day: the day the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Hour: the hour the observation was taken in Coordinated Universal Time (UTC).\n",
    "\n",
    "Latitude: latitude of the station (in decimal degrees). North (+); South (-).\n",
    "\n",
    "Longitude: the longitude of the station (in decimal degrees). East (+); West (-).\n",
    "\n",
    "Temperature: 2 meter (circa) Above Ground Level Air (dry bulb) Temperature (⁰C to tenths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- Raw data was removed in download_ghcn.py for storage purposes.\n",
    "- GHCN hourly dataset contained psv files for individual stations in specific years. When processing the data, it was converted to csv format files for all California stations in years 2003 - 2023.\n",
    "- Most columns were dropped as they were not needed. Columns kept were described above.\n",
    "- Duplicate rows that had completely same column values were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Update paths to get source code from notebook_utils\n",
    "curr_dir = os.path.dirname(os.path.abspath('notebooks'))\n",
    "proj_dir = os.path.dirname(curr_dir)\n",
    "src_path = os.path.join(proj_dir, 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from notebook_utils.preprocessing import *\n",
    "\n",
    "CA_stations = get_reduced_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature column has a really high max celsius value which is 902 degrees celsius. This is unreasonably high. After doing some searching, we found that the highest recorded temperature value was 56.7 degrees celsius in California 1913. \n",
    "\n",
    "There is also an unreasonably low temperature observation of -99 degrees celsius since the lowest recorded temperature observation on Earth was -98 degrees in Antartica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Invalid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle Missing Values  (e.g., mean/median impuation, interpolation, forward or backward fill, k-nearest neighbors imputation, deletion)\n",
    "- Handle Outliers  (e.g., visual inspection by boxplots, Z-score and IQR method, or data transformation by log transformation and winsorization)\n",
    "- Handle inconsistencies (e.g., checking ranges to ensure temperature values fall within a reasonable range, unit consistency, string matching and standardization), and duplicates (identify and remove duplicates) in the dataset\n",
    "\n",
    "Notes:\n",
    "- For non-leap years, there should be 8760 rows (for each hour) for each station.\n",
    "- For leap years, there should be 8784 rows (for each hour) for each station\n",
    "- Leap years from 2003-2023 include: 2004, 2008, 2012, 2016, and 2020\n",
    "- The reduced files contain 99 CA stations.\n",
    "- Some stations are not observed each year from 2003-2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure temperature observations are within -50°C and 60°C\n",
    "2. Temperature values above the reasonable range will be converted to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows are below -50 and above 60 degrees celsius\n",
    "CA_stations[(CA_stations['temperature'] < -50) | (CA_stations['temperature'] > 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to turn temperature outside of the specified range into NaN\n",
    "CA_stations.loc[(CA_stations['temperature'] < -50) | (CA_stations['temperature'] > 60), 'temperature'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are rows with the same value in each column except temperature. In these cases we will average out the temperature observations and delete the extra rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = CA_stations.groupby(['Station_ID', 'Station_name', 'Year', 'Month', 'Day', 'Hour', 'Latitude', 'Longitude']).agg({'temperature': 'mean'}).reset_index()\n",
    "CA_stations = grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reference dataframe with all stations with hours from 2003 to 2023\n",
    "full_df = create_full_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in completely missing rows\n",
    "missing_rows = find_missing_rows(full_df, CA_stations)\n",
    "CA_stations = add_missing_rows(CA_stations, missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = check_station_rows(CA_stations)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations with no Station_name values will be dropped as that means there are no observations recorded for them at all from 2003 - 2023\n",
    "- This is because the above Station_name for missing rows were filled in by using the Station_name used from a filled column with the same Station_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_stations.dropna(subset=['Station_name'], inplace=True)   # 21 stations dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = CA_stations.isna().sum()\n",
    "null_percent = (null_count/CA_stations.shape[0]) * 100\n",
    "print(f'Percentage of null values for each column: \\n{null_percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Temperature Values: Handling large gaps in temperature observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a \"large gap\" as being a gap in the temperature column that is more than a day.\n",
    "\n",
    "Large gaps in temperature observations will be handled via interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubic Spline Interpolation for large gaps of missing temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.preprocessing import cubic_spline_interpolate\n",
    "\n",
    "CA_interpolated_df = cubic_spline_interpolate(CA_stations,gap_hours=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_interpolated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = CA_interpolated_df.isna().sum()\n",
    "null_percent = (null_count/CA_interpolated_df.shape[0]) * 100\n",
    "print(f'Percentage of null values for each column: \\n{null_percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Temperature Values: Handling short gaps in temperature observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a \"short gap\" as being a gap in the temperature column that is less than a day. \n",
    "\n",
    "Short gaps in temperature observations will be handled with forward/backward fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = fill_gaps(CA_interpolated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = final_df.isna().sum()\n",
    "null_percent = (null_count/final_df.shape[0]) * 100\n",
    "print(f'Percentage of null values for each column: \\n{null_percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make copies to not alter original dataframes\n",
    "plot_CA_stations = CA_stations.copy()\n",
    "plot_CA_interpolated = CA_interpolated_df.copy()\n",
    "\n",
    "plot_CA_stations['Datetime'] = pd.to_datetime(plot_CA_stations[['Year', 'Month', 'Day', 'Hour']])\n",
    "plot_CA_interpolated['Datetime'] = pd.to_datetime(plot_CA_interpolated[['Year', 'Month', 'Day', 'Hour']])\n",
    "\n",
    "plot_CA_stations.set_index('Datetime', inplace=True)\n",
    "plot_CA_interpolated.set_index('Datetime', inplace=True)\n",
    "\n",
    "monthly_avg_original = plot_CA_stations['temperature'].resample('ME').mean()\n",
    "monthly_avg_interpolated = plot_CA_interpolated['temperature'].resample('ME').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(monthly_avg_original.index, monthly_avg_original.values, 'o-', label='Original Data', markersize=4)\n",
    "\n",
    "plt.plot(monthly_avg_interpolated.index, monthly_avg_interpolated.values, 'x-', label='Interpolated Data', markersize=4)\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Monthly Average Temperature (2003-2023)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers can skew our statistical analysis of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Visual Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Year', y='temperature', data=CA_interpolated_df)\n",
    "plt.title('Temperature Boxplot by Year')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.ylabel('Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Statistical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# z_stations dataframe will have a Z_score column added to it\n",
    "z_stations = CA_interpolated_df.copy()\n",
    "z_stations['Z_score'] = stats.zscore(z_stations['temperature'])\n",
    "z_thresh = 3\n",
    "\n",
    "# calculate outliers using Z-score\n",
    "z_outliers = z_stations[(z_stations['Z_score'] < -z_thresh) | (z_stations['Z_score'] > z_thresh)]\n",
    "total_observations = z_stations.shape[0]\n",
    "\n",
    "num_outliers = z_outliers.shape[0]\n",
    "\n",
    "percent_outliers = (num_outliers / total_observations) * 100\n",
    "\n",
    "print(f'{percent_outliers:.2f}% of the observations are outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stations_cleaned = z_stations[~((z_stations['Z_score'] < -z_thresh) | (z_stations['Z_score'] > z_thresh))]\n",
    "\n",
    "# Drop the Z_score column as it's no longer needed\n",
    "z_stations_cleaned = z_stations_cleaned.drop(columns=['Z_score'])\n",
    "\n",
    "# The z_stations_cleaned dataframe now contains the data without the outliers\n",
    "z_stations_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTC to Local Time Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stations_cleaned['Datetime'] = pd.to_datetime(z_stations_cleaned[['Year', 'Month', 'Day', 'Hour']])\n",
    "z_stations_cleaned = z_stations_cleaned.drop(columns=['Year', 'Month', 'Day', 'Hour'])\n",
    "\n",
    "z_stations_cleaned['Datetime_local'] = z_stations_cleaned['Datetime'].dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
    "z_stations_cleaned['Year'] = z_stations_cleaned['Datetime_local'].dt.year\n",
    "z_stations_cleaned['Month'] = z_stations_cleaned['Datetime_local'].dt.month\n",
    "z_stations_cleaned['Day'] = z_stations_cleaned['Datetime_local'].dt.day\n",
    "z_stations_cleaned['Hour'] = z_stations_cleaned['Datetime_local'].dt.hour\n",
    "z_stations_cleaned = z_stations_cleaned.drop(columns=['Datetime', 'Datetime_local'])\n",
    "\n",
    "# remove 2002 observations\n",
    "z_stations_cleaned = z_stations_cleaned[z_stations_cleaned['Year'] != 2002]\n",
    "\n",
    "# rearrange columns\n",
    "cols = ['Station_ID','Station_name', 'Latitude', 'Longitude','Year','Month','Day','Hour','temperature']\n",
    "z_stations_cleaned = z_stations_cleaned[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stations_cleaned.head()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to csv files by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../data/processed/ghcn_clean'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "years = z_stations_cleaned['Year'].unique()\n",
    "\n",
    "# Capitalize the temperature column to match the rest.\n",
    "z_stations_cleaned = z_stations_cleaned.rename(columns={'temperature': 'Temperature'})\n",
    "z_stations_cleaned.sort_values(by=['Station_ID', 'Year', 'Month', 'Day', 'Hour'], inplace=True)\n",
    "\n",
    "for year in years:\n",
    "    yearly_data = z_stations_cleaned[z_stations_cleaned['Year'] == year]\n",
    "    output_file = os.path.join(output_folder, f'CA_{year}_clean.csv')\n",
    "    yearly_data.to_csv(output_file, index=False)\n",
    "\n",
    "print('New datafrane yearly files saved to ghcn_cleaned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
